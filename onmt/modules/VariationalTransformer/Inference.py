import numpy as np
import torch, math
import torch.nn as nn
import torch.nn.functional as F
from onmt.modules.Transformer.Layers import PositionalEncoding
from onmt.modules.Transformer.Layers import EncoderLayer, DecoderLayer
from onmt.modules.StochasticTransformer.Layers import StochasticEncoderLayer, StochasticDecoderLayer
from onmt.modules.Transformer.Models import TransformerEncoder, TransformerDecoder
import onmt
from onmt.modules.WordDrop import embedded_dropout
from onmt.modules.Transformer.Layers import XavierLinear, MultiHeadAttention, FeedForward, PrePostProcessing

Linear = XavierLinear

import copy
from onmt.modules.Utilities import mean_with_mask, max_with_mask

"""
    Variational Inference for model depth generation

    Our model structure is generated by a latent variable z = {z_1, z_2 .... z_n} corresponding to n layers
    Assumption is each layer is generated randomly (motivated by the Stochastic Network)
    Mean Field assumption is used (one set of parameters for each z)

    Our loss function is:

    L = E_q_z ( log (p (Y|X, z)) - KL( q(z|X, y) || p(z|X))
    (data likelihood given the latent variable)

    The Prior model estimates p(z | x)

    The Posterior model estimates q(z | x, y)

    During training we take the sample from posterior (variational inference)
    During testing  y is not available, so we use the prior (conditional prior)

"""

"""
    The Prior model estimates p(z | x)
"""


class NeuralPrior(nn.Module):
    """Encoder in 'Attention is all you need'

    Args:
        opt: list of options ( see train.py )
        dicts : dictionary (for source language)

    """

    def __init__(self, opt, embedding, positional_encoder):

        super(NeuralPrior, self).__init__()

        encoder_opt = copy.deepcopy(opt)
        # quick_hack to override some hyper parameters of the prior encoder
        encoder_opt.layers = opt.layers if opt.var_ignore_source else opt.layers // 2
        self.dropout = opt.dropout
        self.opt = opt
        self.pooling = opt.var_pooling

        self.var_ignore_first_source_token = opt.var_ignore_first_source_token

        self.encoder = TransformerEncoder(encoder_opt, embedding, positional_encoder)

        self.projector = Linear(opt.model_size, opt.model_size)
        self.mean_predictor = Linear(opt.model_size, opt.model_size)
        self.var_predictor = Linear(opt.model_size, opt.model_size)

    def forward(self, input, **kwargs):
        """
        Inputs Shapes:
            input: batch_size x len_src (wanna tranpose)

        Outputs Shapes:
            out: batch_size x len_src x d_model
            mask_src

        """
        if self.var_ignore_first_source_token:
            input = input[:, 1:]
        # pass the input to the transformer encoder (we also return the mask)
        freeze_embedding_ = True
        if self.opt.var_ignore_source:
            freeze_embedding_ = False
        context, _ = self.encoder(input, freeze_embedding=freeze_embedding_)

        # Now we have to mask the context with zeros
        # context size: T x B x H
        # mask size: T x B x 1 for broadcasting
        mask = input.eq(onmt.Constants.PAD).transpose(0, 1).unsqueeze(2)

        if self.pooling == 'mean':
            context = mean_with_mask(context, mask)
        elif self.pooling == 'max':
            context = max_with_mask(context, mask)
        encoder_meaning = context
        context = torch.tanh(self.projector(context))

        mean = self.mean_predictor(context)
        log_var = self.var_predictor(context).float()

        var = torch.exp(0.5 * log_var)

        p_z = torch.distributions.normal.Normal(mean.float(), var)

        # return prior distribution P(z | X)
        return encoder_meaning, p_z


class NeuralPosterior(nn.Module):
    """Neural Posterior using Transformer

    Args:
        opt: list of options ( see train.py )
        embedding : dictionary (for target language)

    """

    def __init__(self, opt, embedding, positional_encoder, prior=None):

        super(NeuralPosterior, self).__init__()

        encoder_opt = copy.deepcopy(opt)
        self.opt = opt

        # quick_hack to override some hyper parameters of the prior encoder
        encoder_opt.layers = opt.layers if opt.var_ignore_source else opt.layers // 2
        # encoder_opt.word_dropout = 0.0
        self.dropout = opt.dropout
        self.pooling = opt.var_pooling

        self.var_ignore_first_target_token = opt.var_ignore_first_target_token

        self.posterior_combine = opt.var_posterior_combine
        if opt.var_posterior_combine == 'concat':
            self.projector = Linear(opt.model_size * 2, opt.model_size)
        elif opt.var_posterior_combine == 'sum':
            self.projector = Linear(opt.model_size * 1, opt.model_size)
        else:
            raise NotImplementedError

        if opt.var_posterior_share_weight == True:
            assert prior is not None
            self.encoder = prior.encoder
        else:
            self.encoder = TransformerEncoder(encoder_opt, embedding, positional_encoder)

        self.mean_predictor = Linear(opt.model_size, opt.model_size)
        self.var_predictor = Linear(opt.model_size, opt.model_size)

    def forward(self, encoder_meaning, input_src, input_tgt, **kwargs):
        """
        Inputs Shapes:
            input: batch_size x len_src (wanna tranpose)

        Outputs Shapes:
            out: batch_size x len_src x d_model
            mask_src

        """

        """ Embedding: batch_size x len_src x d_model """

        if self.var_ignore_first_target_token:
            input_tgt = input_tgt[:, 1:]

        # encoder_context = encoder_context.detach()
        decoder_context, _ = self.encoder(input_tgt, freeze_embedding=True)

        # src_mask = input_src.eq(onmt.Constants.PAD).transpose(0, 1).unsqueeze(2)
        tgt_mask = input_tgt.eq(onmt.Constants.PAD).transpose(0, 1).unsqueeze(2)

        # take the mean of each context
        encoder_context = encoder_meaning

        if self.pooling == 'mean':
            decoder_context = mean_with_mask(decoder_context, tgt_mask)
        elif self.pooling == 'max':
            decoder_context = max_with_mask(decoder_context, tgt_mask)

        if self.posterior_combine == 'concat':
            context = torch.cat([encoder_context, decoder_context], dim=-1)
        elif self.posterior_combine == 'sum':
            context = encoder_context + decoder_context

        context = torch.tanh(self.projector(context))

        mean = self.mean_predictor(context)
        log_var = self.var_predictor(context).float()
        var = torch.exp(0.5 * log_var)

        q_z = torch.distributions.normal.Normal(mean.float(), var)

        # return distribution Q(z | X, Y)
        return q_z